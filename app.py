import gradio as gr
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from diffusers import StableDiffusionPipeline
from deep_translator import GoogleTranslator
import torch

# Stable Diffusion Pipeline'i yÃ¼kleme
def load_pipeline():
    try:
        print("Stable Diffusion Pipeline yÃ¼kleniyor...")
        # Modeli Ã¶nbelleÄŸe kaydet ve zaman aÅŸÄ±mÄ± ile yÃ¼kle
        pipe = StableDiffusionPipeline.from_pretrained(
            "CompVis/stable-diffusion-v1-4",
            cache_dir="./cache",  # Modelleri cache'e kaydet
            resume_download=True,  # Ä°ndirme kesilirse devam et
            timeout=120  # Zaman aÅŸÄ±mÄ±nÄ± artÄ±r
        )
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe = pipe.to(device)
        print(f"Model {device.upper()} Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor.")
        return pipe
    except Exception as e:
        print(f"Model yÃ¼kleme sÄ±rasÄ±nda hata oluÅŸtu: {e}")
        return None

pipe = load_pipeline()

# Ã‡eviri fonksiyonu
def translate_to_english(prompt):
    try:
        # TÃ¼rkÃ§e prompt'u Ä°ngilizce'ye Ã§evir
        translated_text = GoogleTranslator(source="turkish", target="english").translate(prompt)
        return translated_text
    except Exception as e:
        print(f"Ã‡eviri sÄ±rasÄ±nda hata: {e}")
        return prompt

# GÃ¶rsel Ã¼retim fonksiyonu
negative_prompt = (
    "violence, explicit content, gore, inappropriate for children, "
    "blood, weapon, nudity"
)

def generate_image(prompt, width, height, translate):
    if pipe is None:
        return "Model yÃ¼klenemedi, lÃ¼tfen tekrar deneyin.", None
    
    if len(prompt) > 200:
        return "Prompt Ã§ok uzun! LÃ¼tfen 200 karakterden kÄ±sa bir ÅŸey girin.", None
    if width > 400 or height > 400:
        return "Boyutlar sÄ±nÄ±rÄ± aÅŸÄ±yor! Maksimum boyut 400x400 olmalÄ±dÄ±r.", None

    try:
        # Ã‡eviri tercihi kontrolÃ¼
        if translate:
            prompt = translate_to_english(prompt)

        # GÃ¶rseli Ã¼ret
        image = pipe(prompt, width=width, height=height, negative_prompt=negative_prompt).images[0]
        return None, image
    except Exception as e:
        print(f"GÃ¶rsel Ã¼retim sÄ±rasÄ±nda hata oluÅŸtu: {e}")
        return "GÃ¶rsel Ã¼retim sÄ±rasÄ±nda bir hata oluÅŸtu.", None

# GPU veya CPU durumunu kontrol eden fonksiyon
def check_device():
    if torch.cuda.is_available():
        return "GPU kullanÄ±lÄ±yor."
    else:
        return "GPU kullanÄ±lmÄ±yor, CPU Ã¼zerinde Ã§alÄ±ÅŸÄ±yor."

# Gradio arayÃ¼zÃ¼
with gr.Blocks(css="body { background-color: #f0f8ff; font-family: Arial, sans-serif; } .gr-button { background-color: #ff7f50; color: white; border: none; }") as demo:
    gr.Markdown("### ğŸŒˆ Stable Diffusion GÃ¶rsel Ãœretim AracÄ±")

    with gr.Row():
        prompt = gr.Textbox(label="Prompt (TÃ¼rkÃ§e)", placeholder="Bir ÅŸey yazÄ±n (max 200 karakter)")
        width = gr.Slider(label="GeniÅŸlik", minimum=100, maximum=400, step=50, value=400)
        height = gr.Slider(label="YÃ¼kseklik", minimum=100, maximum=400, step=50, value=400)

    with gr.Row():
        translate = gr.Checkbox(label="TÃ¼rkÃ§e'den Ä°ngilizce'ye Ã§evir", value=True)

    with gr.Row():
        device_status = gr.Textbox(label="Cihaz Durumu", value=check_device(), interactive=False)

    output_text = gr.Textbox(label="Hata MesajÄ±")
    output_image = gr.Image(label="Ãœretilen GÃ¶rsel")

    generate_button = gr.Button("GÃ¶rsel Ãœret")
    generate_button.click(
        fn=generate_image,
        inputs=[prompt, width, height, translate],
        outputs=[output_text, output_image]
    )

demo.launch(server_name="0.0.0.0", server_port=7860)
