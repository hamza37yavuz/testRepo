import gradio as gr
from diffusers import DiffusionPipeline
import torch
from huggingface_hub import login
import sys
from googletrans import Translator  # Prompt Ã§evirisi iÃ§in

class StableDif: 
    def __init__(self, model_id="stabilityai/stable-diffusion-3-large", devices=None, image_size=128):
        """
        Stable Diffusion 3.5 Large modelini yÃ¼kler ve ayarlar.
        
        Args:
            model_id (str): Hugging Face model kimliÄŸi.
            devices (list): KullanÄ±lacak GPU'larÄ±n listesi (Ã¶r. ["cuda:0", "cuda:1"]).
            image_size (int): GÃ¶rsel boyutu (128, 256 veya 512).
        """
        self.model_id = model_id
        self.devices = devices or ["cuda:0"]  # VarsayÄ±lan olarak tek GPU
        self.image_size = image_size if image_size in [128, 256, 512] else 128  # VarsayÄ±lan 128
        self.pipe = None  # Model baÅŸlangÄ±Ã§ta yÃ¼klenmemiÅŸ olacak

    def _load_model(self, token):
        """
        Modeli yÃ¼kler ve belirtilen GPU'lara daÄŸÄ±tÄ±r.
        
        Args:
            token (str): Hugging Face token'Ä±.
        """
        try:
            print(f"[DEBUG] Model yÃ¼kleniyor: {self.model_id}")
            self.pipe = DiffusionPipeline.from_pretrained(
                self.model_id,
                torch_dtype=torch.float16,
                use_auth_token=token  # Token'Ä± kullanarak modeli yÃ¼kle
            )
            
            # Ã‡oklu GPU kullanÄ±mÄ± (DataParallel)
            if len(self.devices) > 1:
                print(f"[DEBUG] {len(self.devices)} GPU kullanÄ±lÄ±yor: {', '.join(self.devices)}")
                self.pipe.enable_model_cpu_offload()  # Bellek optimizasyonu iÃ§in
            else:
                print(f"[DEBUG] Tek GPU kullanÄ±lÄ±yor: {self.devices[0]}")
                self.pipe = self.pipe.to(self.devices[0])
            
            self.pipe = torch.nn.DataParallel(self.pipe, device_ids=[torch.device(device) for device in self.devices])
            print("[DEBUG] Model baÅŸarÄ±yla yÃ¼klendi!")
        except Exception as e:
            print(f"[ERROR] Model yÃ¼kleme baÅŸarÄ±sÄ±z oldu: {e}")
            raise RuntimeError(f"Model yÃ¼klenemedi: {e}")

    def generate_image(self, prompt, translate_prompt=False):
        """
        Metin girdisine dayalÄ± olarak gÃ¶rsel oluÅŸturur.
        
        Args:
            prompt (str): Ä°stenen gÃ¶rseli tanÄ±mlayan metin.
            translate_prompt (bool): Prompt'u Ä°ngilizceye Ã§evir.
        
        Returns:
            PIL.Image: OluÅŸturulan gÃ¶rsel.
        """
        negative_prompt = (
            "violence, gore, nudity, explicit content, blood, horror, weapons, "
            "disturbing imagery, adult content, offensive material"
        )

        if self.pipe is None:
            raise gr.Error("[ERROR] Model yÃ¼klenmeden gÃ¶rsel oluÅŸturulamaz!")

        try:
            if translate_prompt:
                translator = Translator()
                prompt = translator.translate(prompt, dest="en").text
                print(f"[DEBUG] Ã‡evrilen Prompt: {prompt}")

            print(f"[DEBUG] GÃ¶rsel oluÅŸturuluyor: {prompt}")
            with torch.autocast(self.devices[0]):
                image = self.pipe(
                    prompt,
                    negative_prompt=negative_prompt,
                    height=self.image_size,
                    width=self.image_size
                ).images[0]
            print("[DEBUG] GÃ¶rsel baÅŸarÄ±yla oluÅŸturuldu!")
            return image
        except Exception as e:
            print(f"[ERROR] GÃ¶rsel oluÅŸturulurken hata oluÅŸtu: {e}")
            raise RuntimeError(f"GÃ¶rsel oluÅŸturulamadÄ±: {e}")

def create_gradio_interface(generator):
    """
    Gradio arayÃ¼zÃ¼nÃ¼ oluÅŸturur.
    
    Returns:
        gr.Blocks: Gradio arayÃ¼zÃ¼.
    """
    def generate_image_wrapper(prompt, translate_prompt):
        return generator.generate_image(prompt, translate_prompt)

    custom_css = """
    .gradio-container {
        background: linear-gradient(45deg, #ff9a9e, #fad0c4, #fbc2eb, #a18cd1, #fad0c4, #ffdde1);
    }
    """

    try:
        with gr.Blocks(css=custom_css) as demo:
            gr.Markdown("# ğŸ¨ Stable Diffusion 3.5 Large ile GÃ¶rsel OluÅŸturma ğŸ¨")
            gr.Markdown("### Renkli ve eÄŸlenceli gÃ¶rseller oluÅŸturun!")
            
            with gr.Row():
                with gr.Column():
                    prompt = gr.Textbox(
                        label="Prompt (Ä°stenen GÃ¶rsel)",
                        placeholder="Ã–rneÄŸin: Mutlu bir Ã§ocuk, renkli balonlarla",
                        max_lines=3,
                        info="LÃ¼tfen istediÄŸiniz gÃ¶rseli tanÄ±mlayan bir metin girin."
                    )
                    translate_prompt = gr.Checkbox(
                        label="Prompt'u Ä°ngilizceye Ã‡evir",
                        value=False,
                        info="Ä°ÅŸaretli deÄŸilse, prompt otomatik olarak Ä°ngilizceye Ã§evrilir."
                    )
                    generate_button = gr.Button("GÃ¶rsel OluÅŸtur", variant="primary")
                with gr.Column():
                    output_image = gr.Image(label="OluÅŸturulan GÃ¶rsel", interactive=False)

            generate_button.click(
                generate_image_wrapper,
                inputs=[prompt, translate_prompt],
                outputs=output_image,
            )
        return demo
    except Exception as e:
        print(f"[ERROR] Gradio arayÃ¼zÃ¼ oluÅŸturulamadÄ±: {e}")
        raise RuntimeError(f"Gradio arayÃ¼zÃ¼ baÅŸlatÄ±lamadÄ±: {e}")
      
def main():
    if len(sys.argv) < 2:
        print("KullanÄ±m: python app2.py <HuggingFace_Token> [image_size]")
        sys.exit(1)

    HUGGINGFACE_TOKEN = sys.argv[1]
    IMAGE_SIZE = int(sys.argv[2]) if len(sys.argv) > 2 and sys.argv[2] in ["128", "256", "512"] else 128

    try:
        print("[DEBUG] Hugging Face hesabÄ±na giriÅŸ yapÄ±lÄ±yor...")
        login(token=HUGGINGFACE_TOKEN)
        print("[DEBUG] Hugging Face giriÅŸ baÅŸarÄ±lÄ±!")

        devices = [f"cuda:{i}" for i in range(torch.cuda.device_count())]
        if not devices:
            raise RuntimeError("KullanÄ±labilir GPU bulunamadÄ±.")
        generator = StableDif(devices=devices, image_size=IMAGE_SIZE)
        generator._load_model(HUGGINGFACE_TOKEN)

        demo = create_gradio_interface(generator)
        print("[DEBUG] Gradio arayÃ¼zÃ¼ baÅŸlatÄ±lÄ±yor...")
        demo.launch(server_name="0.0.0.0", server_port=7860)
    except Exception as e:
        print(f"[ERROR] Program baÅŸlatÄ±lamadÄ±: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
